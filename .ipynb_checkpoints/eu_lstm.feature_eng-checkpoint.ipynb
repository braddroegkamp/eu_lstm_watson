{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "sc_f = RobustScaler()\n",
    "sc_y = RobustScaler()\n",
    "\n",
    "def add_price_returns(df, x):\n",
    "    '''\n",
    "    Parameters: Pyspark df with \"Close\" price column, x minute parameter\n",
    "    Returns: Pyspark df with added x price_xmin_return column\n",
    "    '''\n",
    "    df_return = df.withColumn('tmp_lag_price', lag(df.Close, count=x).over(Window.orderBy('Timestamp')))\n",
    "    col_name = 'price_' + str(x) + 'min_return'\n",
    "    df_return = df_return.withColumn(col_name, df_return.Close - df_return.tmp_lag_price).na.drop()\n",
    "    return df_return.drop('tmp_lag_price')\n",
    "\n",
    "def get_dates_times_subset(df, startYear, endYear, st, et):\n",
    "    '''\n",
    "    BE SURE TO INCLUDE ALL NEEDED DATA IN TIMESPAN!  \n",
    "    Building forecast_window and prior time_window from this data should be considered.\n",
    "    Parameters: Pyspark df with Timestamp column, startYear(YYYY), endYear(YYYY), start time (st), end time (et)\n",
    "    Returns: Pyspark df between specified dates, specific times\n",
    "    '''\n",
    "    df_return = df.filter((year('Timestamp') >= lit(startYear)) & (year('Timestamp') <= lit(endYear)))\\\n",
    "                  .filter((hour('Timestamp') >= lit(st.hour)) & (hour('Timestamp') <= lit(et.hour)))\\\n",
    "                  .filter((hour('Timestamp') != lit(st.hour)) | (minute('Timestamp') >= lit(st.minute)))\\\n",
    "                  .filter((hour('Timestamp') != lit(et.hour)) | (minute('Timestamp') <= lit(et.minute)))\n",
    "    return df_return\n",
    "\n",
    "def add_dataset_check_col(df, startTime, endTime):\n",
    "    '''\n",
    "    Parameters: Pyspark df with Timestamp column, startTime(HH:MM), endTime(HH:MM)\n",
    "    Returns: Pyspark df with \"is_in_dataset\" column, identifying rows that stay in dataset after final setup\n",
    "    '''\n",
    "    st = datetime.datetime.strptime(startTime, '%H:%M')\n",
    "    et = datetime.datetime.strptime(endTime, \"%H:%M\")\n",
    "    df_return = df.withColumn('is_in_dataset', ((hour('Timestamp') == lit(st.hour)) & (minute('Timestamp') >= lit(st.minute))) | \\\n",
    "                                        ((hour('Timestamp') == lit(et.hour)) & (minute('Timestamp') <= lit(et.minute))) | \\\n",
    "                                        ((hour('Timestamp') >= lit(min(st.hour + 1, et.hour))) & \\\n",
    "                                         (hour('Timestamp') <= lit(max(et.hour - 1, st.hour)))))\n",
    "    return df_return\n",
    "\n",
    "def get_arrays_for_lstm(df, time_window, forecast_window, batch_size):\n",
    "    '''\n",
    "    Assumes cols 5, 6 are price, volume features, 7 will be y var, and last col is datacheck bool\n",
    "    Parameters: Pyspark df, time_window, forecast_window, batch_size\n",
    "    Returns: Transformed 3D X and y np arrays for LSTM network model\n",
    "    '''\n",
    "    pdf = df.toPandas()\n",
    "    \n",
    "    # only need price returns and volumes, but keep dataset check vals for below\n",
    "    feature_set = pdf.iloc[:, 5:7].values\n",
    "    y_set = pdf.iloc[:, 7:8].values\n",
    "    is_in_dataset_check = pdf.iloc[:, -1]\n",
    "\n",
    "    # feature scaling\n",
    "    feature_set_scaled = sc_f.fit_transform(np.float64(feature_set))\n",
    "    y_set_scaled = sc_y.fit_transform(np.float64(y_set))\n",
    "\n",
    "    # filter data into needed arrays\n",
    "    x_price_train = []\n",
    "    x_volume_train = []\n",
    "    y_train = []\n",
    "\n",
    "    length = len(feature_set_scaled)\n",
    "    for i in range(0, length):\n",
    "        x_volume_train.append(feature_set_scaled[max(0, i - time_window):i, 0])\n",
    "        x_price_train.append(feature_set_scaled[max(0, i - time_window):i, 1])\n",
    "\n",
    "    for i in range(0, len(y_set_scaled)):\n",
    "        y_train.append(y_set_scaled[max(0, i-forecast_window):max(0, i-forecast_window)+1, 0])\n",
    "\n",
    "    # now that we have the time_window data, remove unwanted entries based on prior is_in_dataset_check\n",
    "    x_volume_train, x_price_train, y_train, is_in_dataset_check = \\\n",
    "        np.array(x_volume_train), np.array(x_price_train), np.array(y_train), np.array(is_in_dataset_check)\n",
    "    x_volume_train = x_volume_train[is_in_dataset_check]\n",
    "    x_price_train = x_price_train[is_in_dataset_check]\n",
    "    y_train = y_train[is_in_dataset_check]\n",
    "\n",
    "    # reduce size of dataset to be divisible by batch size\n",
    "    x_volume_train = x_volume_train[0:len(x_volume_train) - len(x_volume_train)%batch_size]\n",
    "    x_price_train = x_price_train[0:len(x_price_train) - len(x_price_train)%batch_size]\n",
    "    y_train = y_train[0:len(y_train) - len(y_train)%batch_size]\n",
    "\n",
    "    # combine and reshape for modeling\n",
    "    x_volume_train = np.reshape(np.array(x_volume_train.tolist()), (x_volume_train.shape[0], 10))\n",
    "    x_price_train = np.reshape(np.array(x_price_train.tolist()), (x_price_train.shape[0], 10))\n",
    "    X_train = np.dstack((x_price_train, x_volume_train))\n",
    "    y_train = np.reshape(np.array(y_train.tolist()), (y_train.shape[0], 1))\n",
    "    print(\"Feature set shape (standardized price & volume w/10min window): \")\n",
    "    print(X_train.shape)\n",
    "    print(X_train[0])\n",
    "    print('\\n')\n",
    "    print(\"y var shape (standardized 5min future price return): \")\n",
    "    print(y_train.shape)\n",
    "    print(y_train[0])\n",
    "    print('\\n')\n",
    "    return X_train, y_train\n",
    "\n",
    "def get_mlr_df(df, time_window, forecast_window):\n",
    "    '''\n",
    "    DOES NOT STANDARDIZE VARIABLES!\n",
    "    Parameters: Pyspark df (with Timestamp, price_1min_return, Volume, and price_5min_return, is_in_dataset cols), \n",
    "                time_window, forecast_window\n",
    "    Returns: Pyspark df with cols \"sma_price\", \"sma_volume\", \"y_price\"\n",
    "    '''\n",
    "    # add sma cols for price returns and volume (features)\n",
    "    df_return = df.withColumn('sma_price', F.avg(\"price_1min_return\")\\\n",
    "                              .over(Window.orderBy('Timestamp').rowsBetween(-time_window, 0)))\n",
    "    df_return = df_return.withColumn('sma_volume', F.avg(\"Volume\")\\\n",
    "                                     .over(Window.orderBy('Timestamp').rowsBetween(-time_window, 0)))\n",
    "    \n",
    "    # shift 5 min return col to line up y var\n",
    "    df_return = df_return.withColumn('label', \\\n",
    "                                     lag(df.price_5min_return, count=-forecast_window).over(Window.orderBy('Timestamp')))\n",
    "\n",
    "    # now that we have the time_window data, remove unwanted entries based on prior is_in_dataset_check\n",
    "    df_return = df_return.filter(df.is_in_dataset)\n",
    "\n",
    "    return df_return.drop('Open', 'High', 'Low', 'Close', 'Volume', 'price_1min_return', 'price_5min_return', 'is_in_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define important data parameters\n",
    "forecast_window = 5\n",
    "time_window = 10\n",
    "batch_size = 64\n",
    "start_time = '09:15'\n",
    "end_time = '11:15'\n",
    "\n",
    "# add price returns columns\n",
    "df = add_price_returns(df, 1)\n",
    "df = add_price_returns(df, 5)\n",
    "\n",
    "# set train data to year 2013-2015 (val data to 2016, test data to 2017)\n",
    "# include only data between needed time windows (start to end time, plus time, forecast windows added accordingly)\n",
    "st = datetime.datetime.strptime(start_time, '%H:%M') - datetime.timedelta(minutes = time_window)\n",
    "et = datetime.datetime.strptime(end_time, '%H:%M') + datetime.timedelta(minutes = forecast_window)\n",
    "df_train = get_dates_times_subset(df, 2013, 2015, st, et)\n",
    "df_val   = get_dates_times_subset(df, 2016, 2016, st, et)\n",
    "df_test  = get_dates_times_subset(df, 2017, 2017, st, et)\n",
    "\n",
    "# add a column to later filter out fields not needed in final dataset.\n",
    "df_train = add_dataset_check_col(df_train, start_time, end_time)\n",
    "df_val   = add_dataset_check_col(df_val, start_time, end_time)\n",
    "df_test  = add_dataset_check_col(df_test, start_time, end_time)\n",
    "\n",
    "df_train.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get arrays for lstm network model\n",
    "X_train, y_train = get_arrays_for_lstm(df_train, time_window, forecast_window, batch_size)\n",
    "X_test, y_test   = get_arrays_for_lstm(df_test, time_window, forecast_window, batch_size)\n",
    "X_val, y_val     = get_arrays_for_lstm(df_val, time_window, forecast_window, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these will be kept in a pySpark dataframe so we can later show off a Pipeline assembly in Spark\n",
    "mlr_train = get_mlr_df(df_train, time_window, forecast_window)\n",
    "mlr_test  = get_mlr_df(df_test, time_window, forecast_window)\n",
    "\n",
    "mlr_train.show(10, False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
