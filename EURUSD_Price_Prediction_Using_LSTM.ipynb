{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "<h1 align=\"center\">Predicting EUR/USD with LSTM Network</h1> \n<h3 align=\"center\">Bradley Droegkamp</h3> ", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "# Introduction\n***\nForex price prediction, much like stock price prediction, is a near impossible task given all the noise involved in price time series data.  However, profitable trading strategies can be made from models that provide only a sliver of edge.  In this project, I will use a Long Short-Term Memory (LSTM - http://colah.github.io/posts/2015-08-Understanding-LSTMs/) network to predict the 5 minute future price of the front month EUR/USD futures contract (EU) listed on the Chicago Mercantile Exchange (CME - https://www.cmegroup.com/trading/fx/g10/euro-fx.html).  The model will focus on a small subset of trading hours (9:15 - 11:15AM CST).\n<br>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Data\n***\nThe data set consists of 1-minute increment front-month EU price data from September 27, 2009 to April 18, 2018, though we will only use a subset. The data was purchased from kibot (www.kibot.com), a vendor of CME intraday data.  Note the data contains all open hours of trading, which is a 23 hour trading day of 17:00 t-1 - 16:00 CST Monday(Sunday PM) to Friday.\n\n### Contract Details\n<table class=\"cmeSpecTable\" summary=\"Contract Specifications Product Table\" cellspacing=\"0\" cellpadding=\"2\">\n<tbody>\n<tr>\n<td class=\"prodSpecAtribute\">Contract Unit</td>\n<td colspan=\"5\" style=\"text-align:left\">125,000 euro</td>\n</tr>\n<tr>\n<td class=\"prodSpecAtribute\" rowspan=\"1\">Trading Hours</td>\n<td colspan=\"3\" style=\"text-align:left\">Sunday - Friday 6:00 p.m. - 5:00 p.m. (5:00 p.m. - 4:00 p.m. Chicago Time/CT) with a 60-minute break each day beginning at 5:00 p.m. (4:00 p.m. CT)</td>\n</tr>\n<tr>\n<td class=\"prodSpecAtribute\" rowspan=\"1\">Minimum Price Fluctuation*</td>\n<td colspan=\"3\" style=\"text-align:left\">Outrights: .00005 USD per EUR increments ($6.25 USD).<br />Consecutive Month Spreads: (Globex only)&nbsp;&nbsp;0.00001 USD per EUR (1.25 USD)<br />All other Spread Combinations: &nbsp;0.00005 USD per EUR (6.25 USD)</td>\n</tr>\n<tr>\n<td class=\"prodSpecAtribute\">Product Code</td>\n<td colspan=\"5\" style=\"text-align:left\">CME Globex: 6E<br />CME ClearPort: EC<br />Clearing: EC</td>\n</tr>\n<tr>\n<td class=\"prodSpecAtribute\" rowspan=\"1\">Listed Contracts</td>\n<td colspan=\"5\" style=\"text-align:left\">Contracts listed for the first 3 consecutive months and 20 months in the March quarterly cycle (Mar, Jun, Sep, Dec)</td>\n</tr>\n<tr>\n<td class=\"prodSpecAtribute\">Settlement Method</td>\n<td colspan=\"5\" style=\"text-align:left\">Deliverable</td>\n</tr>\n<tr>\n<td class=\"prodSpecAtribute\" rowspan=\"1\">Termination Of Trading</td>\n<td colspan=\"5\" style=\"text-align:left\">9:16 a.m. Central Time (CT) on the second business day immediately preceding the third Wednesday of the contract month (usually Monday).</td>\n</tr>\n<tr>\n<td class=\"prodSpecAtribute\">Settlement Procedures</td>\n<td colspan=\"5\" style=\"text-align:left\">Physical Delivery<br /><a href=\"http://www.cmegroup.com/confluence/display/EPICSANDBOX/Euro\" target=\"_blank\">EUR/USD Futures Settlement Procedures&nbsp;</a></td>\n</tr>\n</tbody>\n</table>\n\n*Source:  https://www.cmegroup.com/trading/fx/g10/euro-fx_contract_specifications.html*\n\n**Min Price Fluctuation changed from 0.0001 to 0.00005 on January 11, 2016 (https://www.cmegroup.com/trading/fx/half-tick.html)*\n\n<br>\n\n#### I.  Bring in the raw data.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+----------+-----+------+------+------+------+------+\n|      Date| Time|  Open|  High|   Low| Close|Volume|\n+----------+-----+------+------+------+------+------+\n|09/27/2009|18:00|  1.47|1.4701| 1.469|1.4691|   441|\n|09/27/2009|18:01|1.4691|1.4691|1.4689| 1.469|    29|\n|09/27/2009|18:02| 1.469| 1.469|1.4688|1.4688|    22|\n|09/27/2009|18:03|1.4687|1.4691|1.4687|1.4691|    38|\n|09/27/2009|18:04|1.4692|1.4693|1.4692|1.4692|    20|\n|09/27/2009|18:05|1.4692|1.4693| 1.469|1.4691|    11|\n|09/27/2009|18:06|1.4691|1.4692|1.4689|1.4692|    14|\n|09/27/2009|18:07|1.4691|1.4691| 1.469| 1.469|     6|\n|09/27/2009|18:08| 1.469|1.4691| 1.469|1.4691|     5|\n|09/27/2009|18:09| 1.469|1.4692| 1.469|1.4692|     7|\n|09/27/2009|18:10|1.4692|1.4692|1.4684|1.4685|    81|\n|09/27/2009|18:11|1.4686|1.4687|1.4683|1.4686|    63|\n|09/27/2009|18:12|1.4687|1.4688|1.4686|1.4687|     7|\n|09/27/2009|18:13|1.4687|1.4692|1.4687|1.4691|    25|\n|09/27/2009|18:14| 1.469|1.4691|1.4684|1.4688|    37|\n|09/27/2009|18:15|1.4686|1.4689|1.4686|1.4688|    66|\n|09/27/2009|18:16|1.4688|1.4688|1.4685|1.4686|    29|\n|09/27/2009|18:17|1.4685|1.4687|1.4683|1.4687|    20|\n|09/27/2009|18:18|1.4686|1.4688|1.4686|1.4688|     4|\n|09/27/2009|18:19|1.4687|1.4689|1.4687|1.4688|    10|\n+----------+-----+------+------+------+------+------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "# The code was removed by Watson Studio for sharing."
        }, 
        {
            "source": "#### II.  Combine Date and Time columns.  Also, these times are in EST, but I prefer CST.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+-------------------+------+------+------+------+------+\n|          Timestamp|  Open|  High|   Low| Close|Volume|\n+-------------------+------+------+------+------+------+\n|2016-01-03 17:00:00|1.0884|1.0886|1.0882|1.0883|   215|\n|2016-01-03 17:01:00|1.0884|1.0884|1.0881|1.0882|    48|\n|2016-01-03 17:02:00|1.0883|1.0884|1.0882|1.0883|    37|\n|2016-01-03 17:03:00|1.0884|1.0884|1.0879|1.0879|    51|\n|2016-01-03 17:04:00|1.0878|1.0878|1.0873|1.0874|   133|\n|2016-01-03 17:05:00|1.0874|1.0875|1.0874|1.0875|    47|\n|2016-01-03 17:06:00|1.0875|1.0877|1.0874|1.0876|    24|\n|2016-01-03 17:07:00|1.0876|1.0876|1.0875|1.0876|    10|\n|2016-01-03 17:08:00|1.0876|1.0876|1.0876|1.0876|     3|\n|2016-01-03 17:09:00|1.0875|1.0875|1.0874|1.0874|    40|\n|2016-01-03 17:10:00|1.0875|1.0875|1.0873|1.0875|   146|\n|2016-01-03 17:11:00|1.0875|1.0877|1.0874|1.0876|   195|\n|2016-01-03 17:12:00|1.0877|1.0877|1.0877|1.0877|     6|\n|2016-01-03 17:13:00|1.0876|1.0876|1.0876|1.0876|    10|\n|2016-01-03 17:14:00|1.0877|1.0877|1.0877|1.0877|    23|\n|2016-01-03 17:15:00|1.0877|1.0877|1.0876|1.0876|     3|\n|2016-01-03 17:16:00|1.0877|1.0877|1.0877|1.0877|     8|\n|2016-01-03 17:17:00|1.0877|1.0879|1.0877|1.0879|    21|\n|2016-01-03 17:18:00|1.0878|1.0878|1.0876|1.0877|    29|\n|2016-01-03 17:19:00|1.0877|1.0877|1.0876|1.0876|    37|\n+-------------------+------+------+------+------+------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "from pyspark.sql.functions import unix_timestamp, from_unixtime, concat, col, lit, hour, minute, year, lag\nfrom pyspark.sql.window import Window\n\n# Convert Date and Time columns to Timestamps and combine\ndf_raw_2 = df_raw.select(unix_timestamp(concat(col('Date'), lit(' '), col('Time')), 'MM/dd/yyyy HH:mm')\\\n                   .cast(TimestampType()).alias('Timestamp'),\n                   'Open', 'High', 'Low', 'Close', 'Volume')\n\n# now substract hour from EST timestamps for CST\ndf = df_raw_2.select(from_unixtime(unix_timestamp(col('Timestamp')) - 60 * 60).alias('Timestamp'),\n                    'Open', 'High', 'Low', 'Close', 'Volume')\n\ndf.createOrReplaceTempView('df')\ndf_2016 = spark.sql(\"SELECT * FROM df WHERE Timestamp BETWEEN '2016-01-01' AND '2016-12-31' ORDER BY Timestamp\")\n\ndf_2016.show()\n\n# pandas df for exploring at next step\npdf_plt = df_2016.toPandas()"
        }, 
        {
            "source": "#### III.  Explore Data\n\nAs expected, prices are not stationary.  We will use returns rather than price to get better results from our model.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 4, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": false
            }, 
            "outputs": [], 
            "source": "import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.dates as mdates\nimport matplotlib.pyplot as plt\n\npdf_day = pdf_plt[(pdf_plt['Timestamp'] >= '2016-01-05') & (pdf_plt['Timestamp'] < '2016-01-06')]\n\nfig, ax = plt.subplots(1, 2, figsize=(18, 6))\npdf_day.plot(x=\"Timestamp\", y=\"Close\", ax=ax[0], legend=None)\nax[0].set_xlabel(\"Time\")\nax[0].get_figure().autofmt_xdate()\n#xfmt = mdates.DateFormatter('%d-%m-%y %H:%M:%S')\n#ax[0].xaxis.set_major_formatter(xfmt)\nax[0].set_ylabel(\"Price\")\nax[0].set_title(\"EUR/USD Price Movement on Jan 5, 2016\", fontsize=16)\n#ax2_sub1 = ax[0].twinx()\n#ax2_sub1.bar(pdf_day.index, pdf_day['Volume'], color='g', alpha=0.5)\n\npdf_plt.plot(x=\"Timestamp\", y=\"Close\", ax=ax[1], legend=None)\nax[1].set_xlabel(\"Time\")\nax[1].set_ylabel(\"Price\")\nax[1].set_title(\"EUR/USD Price Movement in 2016\", fontsize=16)\nax[1].get_figure().autofmt_xdate()"
        }, 
        {
            "source": "Volumes are more concentrated during US daytime hours.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 5, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# TODO:  \n\n# 1. INSERT BAR CHART HERE WITH AVG VOLUMES BY HOUR\n#problem below is df has timestamp as object.  need this to be datetime64 (prefer without slicing warning)\n#times = pd.to_datetime(df.timestamp_col)\n#df.groupby([times.hour, times.minute]).value_col.sum()\n\n# 2. FORMAT X-axis ABOVE (and add volume overlay?).  Prettify with candlesticks???\n\n"
        }, 
        {
            "source": "#### IV.  Set up data for next steps\n\n##### Data will be filtered to 9:15 - 11:15AM CST\n - Common economic releases between 7:15 - 9:00AM CST may bias data if included.\n - Train data set will be 2015, test data 2016, and 2017 for more validation data explained later.\n - These are generally periods of higher volume and volatility.\n - This can be revisited for building models with specific time slots.\n\n<br>\n\nAt this point, we will define:\n - **Time window** used in the LSTM = **10** minutes\n - **Batch size** = **64**\n - **Forecast window** = **5** minutes", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 106, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+-------------------+------+------+------+------+------+--------------------+--------------------+-------------+\n|          Timestamp|  Open|  High|   Low| Close|Volume|   price_1min_return|   price_5min_return|is_in_dataset|\n+-------------------+------+------+------+------+------+--------------------+--------------------+-------------+\n|2015-01-02 09:05:00|1.2024|1.2024|1.2022|1.2024|   515|                 0.0|-0.00100000000000...|        false|\n|2015-01-02 09:06:00|1.2023|1.2025|1.2022|1.2024|   495|                 0.0|-0.00110000000000...|        false|\n|2015-01-02 09:07:00|1.2024|1.2029|1.2022|1.2029|   355| 5.00000000000167E-4|-5.99999999999933...|        false|\n|2015-01-02 09:08:00|1.2028| 1.203|1.2028| 1.203|   425|9.999999999998899E-5|                 0.0|        false|\n|2015-01-02 09:09:00| 1.203|1.2037| 1.203|1.2035|   587|4.999999999999449E-4|0.001100000000000101|        false|\n|2015-01-02 09:10:00|1.2036|1.2038|1.2036|1.2036|   246|9.999999999998899E-5|0.001200000000000...|        false|\n|2015-01-02 09:11:00|1.2036| 1.204|1.2036|1.2037|   803|9.999999999998899E-5|0.001300000000000...|        false|\n|2015-01-02 09:12:00|1.2037| 1.204|1.2037|1.2039|   235|1.999999999999779...|9.999999999998899E-4|        false|\n|2015-01-02 09:13:00|1.2038|1.2038|1.2032|1.2032|   547|-6.99999999999922...|1.999999999999779...|        false|\n|2015-01-02 09:14:00|1.2032|1.2032|1.2028|1.2029|   319|-2.99999999999966...|-5.99999999999933...|        false|\n|2015-01-02 09:15:00|1.2028|1.2028|1.2024|1.2025|   693|-4.00000000000178E-4|-0.00110000000000...|         true|\n|2015-01-02 09:16:00|1.2026|1.2028|1.2022|1.2028|   933| 3.00000000000189E-4|-8.99999999999900...|         true|\n|2015-01-02 09:17:00|1.2028|1.2028|1.2023|1.2023|   273|-5.00000000000167E-4|-0.00160000000000...|         true|\n|2015-01-02 09:18:00|1.2023|1.2025|1.2022|1.2023|   352|                 0.0|-9.00000000000122...|         true|\n|2015-01-02 09:19:00|1.2022|1.2024| 1.202|1.2023|   480|                 0.0|-6.00000000000156E-4|         true|\n|2015-01-02 09:20:00|1.2022|1.2023|1.2022|1.2022|   192|-9.99999999999889...|-2.99999999999966...|         true|\n|2015-01-02 09:21:00|1.2022|1.2022|1.2021|1.2021|   194|-9.99999999999889...|-7.00000000000145E-4|         true|\n|2015-01-02 09:22:00|1.2021|1.2024|1.2021|1.2023|   393|1.999999999999779...|                 0.0|         true|\n|2015-01-02 09:23:00|1.2023|1.2028|1.2023|1.2027|   290| 4.00000000000178E-4| 4.00000000000178E-4|         true|\n|2015-01-02 09:24:00|1.2027|1.2029|1.2027|1.2029|   353|1.999999999999779...| 6.00000000000156E-4|         true|\n+-------------------+------+------+------+------+------+--------------------+--------------------+-------------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "# define important data parameters\nforecast_window = 5\ntime_window = 10\nbatch_size = 64\n\n# add price returns column\ndf_train = df.withColumn('tmp_lag_price', lag(df.Close).over(Window.orderBy('Timestamp')))\ndf_train = df_train.withColumn('price_1min_return', df_train.Close - df_train.tmp_lag_price).na.drop()\ndf_train = df_train.drop('tmp_lag_price')\n\n# add 5 minute return column\ndf_train = df_train.withColumn('tmp_long_lag_price', lag(df.Close, count=forecast_window).over(Window.orderBy('Timestamp')))\ndf_train = df_train.withColumn('price_5min_return', df_train.Close - df_train.tmp_long_lag_price).na.drop()\ndf_train = df_train.drop('tmp_long_lag_price')\n\n# set train data to year 2015 (test data to 2016, and set aside 2017 data for some later validation)\n# include only data between 9:05 and 11:20AM CST.\ndf_train = df_train.filter(year('Timestamp') == lit(2015))\\\n                   .filter((hour('Timestamp') >= lit(9)) & (hour('Timestamp') <= lit(11)))\\\n                   .filter((hour('Timestamp') != lit(9)) | (minute('Timestamp') >= lit(5)))\\\n                   .filter((hour('Timestamp') != lit(11)) | (minute('Timestamp') <= lit(20)))\n\n# add a column to later filter out fields not needed in final dataset.\ndf_train = df_train.withColumn('is_in_dataset', ((hour('Timestamp') == lit(9)) & (minute('Timestamp') >= lit(15))) | \\\n                                                ((hour('Timestamp') == lit(11)) & (minute('Timestamp') <= lit(15))) | \\\n                                                ((hour('Timestamp') == lit(10))))\n\ndf_train.show()"
        }, 
        {
            "source": "##### Set up data using Close price and Volume as features", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 111, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Feature set shape (standardized price & volume w/10min window): \n(31232, 10, 2)\n[[ 0.00e+00  5.15e+02]\n [ 0.00e+00  4.95e+02]\n [ 5.00e-04  3.55e+02]\n [ 1.00e-04  4.25e+02]\n [ 5.00e-04  5.87e+02]\n [ 1.00e-04  2.46e+02]\n [ 1.00e-04  8.03e+02]\n [ 2.00e-04  2.35e+02]\n [-7.00e-04  5.47e+02]\n [-3.00e-04  3.19e+02]]\n\n\ny var shape (standardized 5min future price return): \n(31232, 1)\n[-0.0003]\n"
                }
            ], 
            "source": "from sklearn.preprocessing import MinMaxScaler\n\npdf_train = df_train.toPandas()\n\n# only need price returns and volumes, but keep dataset check vals for below\ntrain_set = pdf_train.iloc[:, 5:8].values\nis_in_dataset_check = pdf_train.iloc[:, -1]\n\n# feature scaling\nsc = MinMaxScaler(feature_range = (0, 1))\ntrain_set_scaled = sc.fit_transform(np.float64(train_set))\ntrain_set_scaled = train_set\n\n# filter data into needed arrays\nx_price_train = []\nx_volume_train = []\ny_train = []\n\nlength = len(train_set_scaled)\nfor i in range(0, length):\n    x_volume_train.append(train_set_scaled[max(0, i - time_window):i, 0])\n    x_price_train.append(train_set_scaled[max(0, i - time_window):i, 1])\n\n\nfor i in range(0, len(train_set_scaled)):\n    y_train.append(train_set_scaled[min(length, i+forecast_window):min(length, i+forecast_window)+1, 2])\n\n# now that we have the time_window data, remove unwanted entries based on prior is_in_dataset_check\nx_volume_train, x_price_train, y_train, is_in_dataset_check = \\\n    np.array(x_volume_train), np.array(x_price_train), np.array(y_train), np.array(is_in_dataset_check)\nx_volume_train = x_volume_train[is_in_dataset_check]\nx_price_train = x_price_train[is_in_dataset_check]\ny_train = y_train[is_in_dataset_check]\n\n# reduce size of dataset to be divisible by batch size\nx_volume_train = x_volume_train[0:len(x_volume_train) - len(x_volume_train)%batch_size]\nx_price_train = x_price_train[0:len(x_price_train) - len(x_price_train)%batch_size]\ny_train = y_train[0:len(y_train) - len(y_train)%batch_size]\n\n# combine and reshape for modeling\nx_volume_train = np.reshape(np.array(x_volume_train.tolist()), (x_volume_train.shape[0], 10))\nx_price_train = np.reshape(np.array(x_price_train.tolist()), (x_price_train.shape[0], 10))\nX_train = np.dstack((x_price_train, x_volume_train))\ny_train = np.reshape(np.array(y_train.tolist()), (y_train.shape[0], 1))\nprint(\"Feature set shape (standardized price & volume w/10min window): \")\nprint(X_train.shape)\nprint(X_train[0])\nprint('\\n')\nprint(\"y var shape (standardized 5min future price return): \")\nprint(y_train.shape)\nprint(y_train[0])\n\n# TODO define test, sim sets\n# vars for MLR"
        }, 
        {
            "source": "# Methodology\n***\n### I.  Fit the LSTM network model", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 112, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Epoch 1/10\n31232/31232 [==============================] - 15s 479us/step - loss: 7.8350e-04\nEpoch 2/10\n31232/31232 [==============================] - 14s 452us/step - loss: 4.5229e-04\nEpoch 3/10\n31232/31232 [==============================] - 14s 459us/step - loss: 4.4697e-04\nEpoch 4/10\n31232/31232 [==============================] - 14s 455us/step - loss: 4.4796e-04\nEpoch 5/10\n31232/31232 [==============================] - 14s 462us/step - loss: 4.4739e-04\nEpoch 6/10\n31232/31232 [==============================] - 15s 465us/step - loss: 4.4594e-04\nEpoch 7/10\n31232/31232 [==============================] - 15s 466us/step - loss: 4.4616e-04\nEpoch 8/10\n31232/31232 [==============================] - 15s 465us/step - loss: 4.4799e-04\nEpoch 9/10\n31232/31232 [==============================] - 15s 467us/step - loss: 4.4710e-04\nEpoch 10/10\n31232/31232 [==============================] - 15s 464us/step - loss: 4.4493e-04\n"
                }, 
                {
                    "execution_count": 112, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "<keras.callbacks.History at 0x2addcfed7940>"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "from keras.layers import Dense, Dropout, Input, LSTM\nfrom keras.models import Sequential, load_model\nimport h5py\n\nlstm = Sequential()\nlstm.add(LSTM(40, batch_input_shape=(batch_size,time_window,2), return_sequences=True, recurrent_dropout = 0.1))\nlstm.add(LSTM(30, recurrent_dropout = 0.2))\nlstm.add(Dropout(0.2))\nlstm.add(Dense(20, activation='relu'))\nlstm.add(Dropout(0.2))\nlstm.add(Dense(5, activation='relu'))\nlstm.add(Dense(1))\nlstm.compile(loss= 'mae', optimizer= 'adam')\nlstm.fit(X_train, y_train, epochs=10, batch_size=batch_size, shuffle=True)\n#set test data in fit?  Why?  Make test data!!!\n\nlstm.save(filepath=\"lstm.h5\")"
        }, 
        {
            "execution_count": 123, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# can skip above and load model last model from here to save time\nlstm = load_model('lstm.h5')"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark", 
            "name": "python3", 
            "language": "python3"
        }
    }, 
    "nbformat": 4
}