{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "<h1 align=\"center\">Predicting EUR/USD with LSTM Network</h1> \n<h3 align=\"center\">Bradley Droegkamp</h3> ", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "# Introduction\n***\nForex price prediction, much like stock price prediction, is a near impossible task given all the noise involved in price time series data.  However, profitable trading strategies can be made from models that provide only a sliver of edge.  In this project, I will use a Long Short-Term Memory (LSTM - http://colah.github.io/posts/2015-08-Understanding-LSTMs/) network to predict the 5 minute future price of the front month EUR/USD futures contract (EU) listed on the Chicago Mercantile Exchange (CME - https://www.cmegroup.com/trading/fx/g10/euro-fx.html).\n<br>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Data\n***\nThe data set consists of 1-minute increment front-month EU price data from September 27, 2009 to April 18, 2018. I had purchased this data from kibot (www.kibot.com), a vendor of CME intraday data.  Note the data contains all open hours of trading, which is a 23 hour trading day of 17:00 t-1 - 16:00 CST Monday(Sunday PM) to Friday.\n<br>\n#### First let's bring in the raw data.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 75, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+----------+-----+------+------+------+------+------+\n|      Date| Time|  Open|  High|   Low| Close|Volume|\n+----------+-----+------+------+------+------+------+\n|09/27/2009|18:00|  1.47|1.4701| 1.469|1.4691|   441|\n|09/27/2009|18:01|1.4691|1.4691|1.4689| 1.469|    29|\n|09/27/2009|18:02| 1.469| 1.469|1.4688|1.4688|    22|\n|09/27/2009|18:03|1.4687|1.4691|1.4687|1.4691|    38|\n|09/27/2009|18:04|1.4692|1.4693|1.4692|1.4692|    20|\n|09/27/2009|18:05|1.4692|1.4693| 1.469|1.4691|    11|\n|09/27/2009|18:06|1.4691|1.4692|1.4689|1.4692|    14|\n|09/27/2009|18:07|1.4691|1.4691| 1.469| 1.469|     6|\n|09/27/2009|18:08| 1.469|1.4691| 1.469|1.4691|     5|\n|09/27/2009|18:09| 1.469|1.4692| 1.469|1.4692|     7|\n|09/27/2009|18:10|1.4692|1.4692|1.4684|1.4685|    81|\n|09/27/2009|18:11|1.4686|1.4687|1.4683|1.4686|    63|\n|09/27/2009|18:12|1.4687|1.4688|1.4686|1.4687|     7|\n|09/27/2009|18:13|1.4687|1.4692|1.4687|1.4691|    25|\n|09/27/2009|18:14| 1.469|1.4691|1.4684|1.4688|    37|\n|09/27/2009|18:15|1.4686|1.4689|1.4686|1.4688|    66|\n|09/27/2009|18:16|1.4688|1.4688|1.4685|1.4686|    29|\n|09/27/2009|18:17|1.4685|1.4687|1.4683|1.4687|    20|\n|09/27/2009|18:18|1.4686|1.4688|1.4686|1.4688|     4|\n|09/27/2009|18:19|1.4687|1.4689|1.4687|1.4688|    10|\n+----------+-----+------+------+------+------+------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "# The code was removed by Watson Studio for sharing."
        }, 
        {
            "source": "#### Combine Date and Time columns.  Also, these times are in EST, but I prefer CST.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 87, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+-------------------+------+------+------+------+------+\n|          Timestamp|  Open|  High|   Low| Close|Volume|\n+-------------------+------+------+------+------+------+\n|2009-09-27 17:00:00|  1.47|1.4701| 1.469|1.4691|   441|\n|2009-09-27 17:01:00|1.4691|1.4691|1.4689| 1.469|    29|\n|2009-09-27 17:02:00| 1.469| 1.469|1.4688|1.4688|    22|\n|2009-09-27 17:03:00|1.4687|1.4691|1.4687|1.4691|    38|\n|2009-09-27 17:04:00|1.4692|1.4693|1.4692|1.4692|    20|\n|2009-09-27 17:05:00|1.4692|1.4693| 1.469|1.4691|    11|\n|2009-09-27 17:06:00|1.4691|1.4692|1.4689|1.4692|    14|\n|2009-09-27 17:07:00|1.4691|1.4691| 1.469| 1.469|     6|\n|2009-09-27 17:08:00| 1.469|1.4691| 1.469|1.4691|     5|\n|2009-09-27 17:09:00| 1.469|1.4692| 1.469|1.4692|     7|\n+-------------------+------+------+------+------+------+\n\n"
                }
            ], 
            "source": "from pyspark.sql.functions import unix_timestamp, from_unixtime, concat, col, lit\n\n# Convert Date and Time columns to Timestamps and combine\ndf_raw_2 = df_raw.select(unix_timestamp(concat(col('Date'), lit(' '), col('Time')), 'MM/dd/yyyy HH:mm')\\\n                   .cast(TimestampType()).alias('Timestamp'),\n                   'Open', 'High', 'Low', 'Close', 'Volume')\n\n# now substract hour from EST timestamps for CST\ndf = df_raw_2.select(from_unixtime(unix_timestamp(col('Timestamp')) - 60 * 60).alias('Timestamp'),\n                    'Open', 'High', 'Low', 'Close', 'Volume')\n\ndf.createOrReplaceTempView('df')\nspark.sql(\"SELECT * FROM df ORDER BY Timestamp LIMIT 10\").show()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark", 
            "name": "python3", 
            "language": "python3"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}